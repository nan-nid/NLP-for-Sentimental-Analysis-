{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**CNN**"
      ],
      "metadata": {
        "id": "5U4r3oOTkd6e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKTMq8LHkSA5",
        "outputId": "b857b5b5-5225-4c46-d7f2-b76a9337acb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "687/687 [==============================] - 37s 52ms/step - loss: -14284.7930 - accuracy: 0.4081 - val_loss: -83396.0234 - val_accuracy: 0.4284\n",
            "Epoch 2/5\n",
            "687/687 [==============================] - 34s 49ms/step - loss: -604970.7500 - accuracy: 0.4157 - val_loss: -1524840.0000 - val_accuracy: 0.4120\n",
            "Epoch 3/5\n",
            "687/687 [==============================] - 34s 50ms/step - loss: -3863259.5000 - accuracy: 0.4152 - val_loss: -6843438.5000 - val_accuracy: 0.4148\n",
            "Epoch 4/5\n",
            "687/687 [==============================] - 33s 48ms/step - loss: -12551910.0000 - accuracy: 0.4174 - val_loss: -18781356.0000 - val_accuracy: 0.4111\n",
            "Epoch 5/5\n",
            "687/687 [==============================] - 34s 49ms/step - loss: -29207040.0000 - accuracy: 0.4166 - val_loss: -39648268.0000 - val_accuracy: 0.4170\n",
            "172/172 [==============================] - 2s 11ms/step - loss: -39648268.0000 - accuracy: 0.4170\n",
            "Test Accuracy: 0.4169546961784363\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv(\"Tweets.csv\")\n",
        "\n",
        "# Preprocessing\n",
        "texts = data['text'].astype(str)\n",
        "labels = data['sentiment']\n",
        "\n",
        "# Convert labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Tokenization\n",
        "max_words = 10000  # Maximum number of words to keep based on word frequency\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding sequences to have consistent length\n",
        "maxlen = 100  # Maximum length of each sequence\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Architecture\n",
        "embedding_dim = 100\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN**"
      ],
      "metadata": {
        "id": "8UNapBrB4QgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv(\"Tweets.csv\")\n",
        "\n",
        "# Preprocessing\n",
        "texts = data['text'].astype(str)\n",
        "labels = data['sentiment']\n",
        "\n",
        "# Convert labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Tokenization\n",
        "max_words = 10000  # Maximum number of words to keep based on word frequency\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding sequences to have consistent length\n",
        "maxlen = 100  # Maximum length of each sequence\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Architecture\n",
        "embedding_dim = 100\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(SimpleRNN(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB8DN10M4SQ1",
        "outputId": "df7bccdb-a748-491b-c8b2-bca86baf5d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "687/687 [==============================] - 35s 49ms/step - loss: -0.4641 - accuracy: 0.4225 - val_loss: -1.7851 - val_accuracy: 0.4455\n",
            "Epoch 2/5\n",
            "687/687 [==============================] - 34s 49ms/step - loss: -3.6159 - accuracy: 0.4793 - val_loss: -5.0758 - val_accuracy: 0.4828\n",
            "Epoch 3/5\n",
            "687/687 [==============================] - 33s 48ms/step - loss: -8.6398 - accuracy: 0.5083 - val_loss: -9.8196 - val_accuracy: 0.4981\n",
            "Epoch 4/5\n",
            "687/687 [==============================] - 35s 52ms/step - loss: -14.4128 - accuracy: 0.4927 - val_loss: -15.5171 - val_accuracy: 0.4452\n",
            "Epoch 5/5\n",
            "687/687 [==============================] - 35s 51ms/step - loss: -20.3161 - accuracy: 0.4863 - val_loss: -19.6569 - val_accuracy: 0.4783\n",
            "172/172 [==============================] - 2s 11ms/step - loss: -19.6569 - accuracy: 0.4783\n",
            "Test Accuracy: 0.47826087474823\n"
          ]
        }
      ]
    }
  ]
}